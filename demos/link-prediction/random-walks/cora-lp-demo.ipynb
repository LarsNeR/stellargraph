{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction with Node2Vec on Cora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "CloudRunner"
    ]
   },
   "source": [
    "<table><tr><td>Run the master version of this notebook:</td><td><a href=\"https://mybinder.org/v2/gh/stellargraph/stellargraph/master?urlpath=lab/tree/demos/link-prediction/random-walks/cora-lp-demo.ipynb\" alt=\"Open In Binder\" target=\"_parent\"><img src=\"https://mybinder.org/badge_logo.svg\"/></a></td><td><a href=\"https://colab.research.google.com/github/stellargraph/stellargraph/blob/master/demos/link-prediction/random-walks/cora-lp-demo.ipynb\" alt=\"Open In Colab\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo notebook demonstrates how to predict citation links/edges between papers using Node2Vec on the Cora dataset.\n",
    "\n",
    "We're going to tackle link prediction as a supervised learning problem on top of node representations/embeddings. The embeddings are computed with the unsupervised node2vec algorithm. After obtaining embeddings, a binary classifier can be used to predict a link, or not, between any two nodes in the graph. Various hyperparameters could be relevant in obtaining the best link classifier - this demo demonstrates incorporating model selection into the pipeline for choosing the best binary operator to apply on a pair of node embeddings.\n",
    "\n",
    "There are four steps:\n",
    "\n",
    "1. Obtain embeddings for each node\n",
    "2. For each set of hyperparameters, train a classifier\n",
    "3. Select the classifier that performs the best\n",
    "4. Evaluate the selected classifier on unseen data to validate its ability to generalise\n",
    "\n",
    "<a name=\"refs\"></a>\n",
    "**References:** \n",
    "\n",
    "[1] Node2Vec: Scalable Feature Learning for Networks. A. Grover, J. Leskovec. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "CloudRunner"
    ]
   },
   "outputs": [],
   "source": [
    "# install StellarGraph if running on Google Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  %pip install -q stellargraph[demos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stellargraph import StellarGraph, datasets\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "The Cora dataset is a homogeneous network where all nodes are papers and edges between nodes are citation links, e.g. paper A cites paper B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.Cora()\n",
    "display(HTML(dataset.description))\n",
    "graph, _ = dataset.load(largest_connected_component_only=True, str_node_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 2485, Edges: 5209\n",
      "\n",
      " Node types:\n",
      "  paper: [2485]\n",
      "    Features: float32 vector, length 1433\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [5209]\n"
     ]
    }
   ],
   "source": [
    "print(graph.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct splits of the input data\n",
    "\n",
    "We have to carefully split the data to avoid data leakage and evaluate the algorithms correctly:\n",
    "\n",
    "* For computing node embeddings, a **Train Graph** (`graph_train`)\n",
    "* For training classifiers, a classifier **Training Set** (`examples_train`) of positive and negative edges that weren't used for computing node embeddings\n",
    "* For choosing the best classifier, an **Model Selection Test Set** (`examples_model_selection`) of positive and negative edges that weren't used for computing node embeddings or training the classifier \n",
    "* For the final evaluation, a **Test Graph** (`graph_test`) to compute test node embeddings with more edges than the Train Graph, and a **Test Set** (`examples_test`) of positive and negative edges not used for neither computing the test node embeddings or for classifier training or model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test Graph\n",
    "\n",
    "We begin with the full graph and use the `EdgeSplitter` class to produce:\n",
    "\n",
    "* Test Graph\n",
    "* Test set of positive/negative link examples\n",
    "\n",
    "The Test Graph is the reduced graph we obtain from removing the test set of links from the full graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Sampled 520 positive and 520 negative edges. **\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 2485, Edges: 4689\n",
      "\n",
      " Node types:\n",
      "  paper: [2485]\n",
      "    Features: float32 vector, length 1433\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [4689]\n"
     ]
    }
   ],
   "source": [
    "# Define an edge splitter on the original graph:\n",
    "edge_splitter_test = EdgeSplitter(graph)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from graph, and obtain the\n",
    "# reduced graph graph_test with the sampled links removed:\n",
    "graph_test, examples_test, labels_test = edge_splitter_test.train_test_split(\n",
    "    p=0.1, method=\"global\"\n",
    ")\n",
    "\n",
    "print(graph_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Graph\n",
    "\n",
    "This time, we use the `EdgeSplitter` on the Test Graph, and perform a train/test split on the examples to produce:\n",
    "\n",
    "* Train Graph\n",
    "* Training set of link examples\n",
    "* Set of link examples for model selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Sampled 468 positive and 468 negative edges. **\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 2485, Edges: 4221\n",
      "\n",
      " Node types:\n",
      "  paper: [2485]\n",
      "    Features: float32 vector, length 1433\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [4221]\n"
     ]
    }
   ],
   "source": [
    "# Do the same process to compute a training subset from within the test graph\n",
    "edge_splitter_train = EdgeSplitter(graph_test, graph)\n",
    "graph_train, examples, labels = edge_splitter_train.train_test_split(\n",
    "    p=0.1, method=\"global\"\n",
    ")\n",
    "(\n",
    "    examples_train,\n",
    "    examples_model_selection,\n",
    "    labels_train,\n",
    "    labels_model_selection,\n",
    ") = train_test_split(examples, labels, train_size=0.75, test_size=0.25)\n",
    "\n",
    "print(graph_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a summary of the different splits that have been created in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Examples</th>\n",
       "      <th>Hidden from</th>\n",
       "      <th>Picked from</th>\n",
       "      <th>Use</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Set</th>\n",
       "      <td>702</td>\n",
       "      <td>Train Graph</td>\n",
       "      <td>Test Graph</td>\n",
       "      <td>Train the Link Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Selection</th>\n",
       "      <td>234</td>\n",
       "      <td>Train Graph</td>\n",
       "      <td>Test Graph</td>\n",
       "      <td>Select the best Link Classifier model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test set</th>\n",
       "      <td>1040</td>\n",
       "      <td>Test Graph</td>\n",
       "      <td>Full Graph</td>\n",
       "      <td>Evaluate the best Link Classifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Number of Examples  Hidden from Picked from  \\\n",
       "Split                                                          \n",
       "Training Set                    702  Train Graph  Test Graph   \n",
       "Model Selection                 234  Train Graph  Test Graph   \n",
       "Test set                       1040   Test Graph  Full Graph   \n",
       "\n",
       "                                                   Use  \n",
       "Split                                                   \n",
       "Training Set                 Train the Link Classifier  \n",
       "Model Selection  Select the best Link Classifier model  \n",
       "Test set             Evaluate the best Link Classifier  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        (\n",
    "            \"Training Set\",\n",
    "            len(examples_train),\n",
    "            \"Train Graph\",\n",
    "            \"Test Graph\",\n",
    "            \"Train the Link Classifier\",\n",
    "        ),\n",
    "        (\n",
    "            \"Model Selection\",\n",
    "            len(examples_model_selection),\n",
    "            \"Train Graph\",\n",
    "            \"Test Graph\",\n",
    "            \"Select the best Link Classifier model\",\n",
    "        ),\n",
    "        (\n",
    "            \"Test set\",\n",
    "            len(examples_test),\n",
    "            \"Test Graph\",\n",
    "            \"Full Graph\",\n",
    "            \"Evaluate the best Link Classifier\",\n",
    "        ),\n",
    "    ],\n",
    "    columns=(\"Split\", \"Number of Examples\", \"Hidden from\", \"Picked from\", \"Use\"),\n",
    ").set_index(\"Split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec\n",
    "\n",
    "We use Node2Vec [[1]](#refs), to calculate node embeddings. These embeddings are learned in such a way to ensure that nodes that are close in the graph remain close in the embedding space. Node2Vec first involves running random walks on the graph to obtain our context pairs, and using these to train a Word2Vec model.\n",
    "\n",
    "These are the set of parameters we can use:\n",
    "\n",
    "* `p` - Random walk parameter \"p\"\n",
    "* `q` - Random walk parameter \"q\"\n",
    "* `dimensions` - Dimensionality of node2vec embeddings\n",
    "* `num_walks` - Number of walks from each node\n",
    "* `walk_length` - Length of each random walk\n",
    "* `window_size` - Context window size for Word2Vec\n",
    "* `num_iter` - number of SGD iterations (epochs)\n",
    "* `workers` - Number of workers for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1.0\n",
    "q = 1.0\n",
    "dimensions = 128\n",
    "num_walks = 10\n",
    "walk_length = 80\n",
    "window_size = 10\n",
    "num_iter = 1\n",
    "workers = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.data import BiasedRandomWalk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "def node2vec_embedding(graph, name):\n",
    "    rw = BiasedRandomWalk(graph)\n",
    "    walks = rw.run(graph.nodes(), n=num_walks, length=walk_length, p=p, q=q)\n",
    "    print(f\"Number of random walks for '{name}': {len(walks)}\")\n",
    "\n",
    "    model = Word2Vec(\n",
    "        walks,\n",
    "        size=dimensions,\n",
    "        window=window_size,\n",
    "        min_count=0,\n",
    "        sg=1,\n",
    "        workers=workers,\n",
    "        iter=num_iter,\n",
    "    )\n",
    "\n",
    "    def get_embedding(u):\n",
    "        return model.wv[u]\n",
    "\n",
    "    return get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         18051866 function calls (18051863 primitive calls) in 7.369 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   392630    1.401    0.000    3.692    0.000 explorer.py:298(naive_weighted_choices)\n",
      "   397600    1.251    0.000    2.871    0.000 graph.py:587(neighbors)\n",
      "  4422328    1.028    0.000    1.028    0.000 explorer.py:435(transition_probability)\n",
      "  4814958    0.936    0.000    1.963    0.000 explorer.py:464(<genexpr>)\n",
      "        1    0.737    0.737    7.348    7.348 explorer.py:359(run)\n",
      "   397600    0.547    0.000    0.547    0.000 graph.py:558(_transform_edges)\n",
      "   397600    0.507    0.000    0.507    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "  4819928    0.324    0.000    0.324    0.000 {method 'append' of 'list' objects}\n",
      "   397600    0.183    0.000    0.736    0.000 <__array_function__ internals>:2(where)\n",
      "   397600    0.179    0.000    0.337    0.000 element_data.py:370(edge_ilocs)\n",
      "   397600    0.111    0.000    0.111    0.000 {method 'get' of 'dict' objects}\n",
      "   397600    0.047    0.000    0.047    0.000 element_data.py:343(_adj_lookup)\n",
      "   397600    0.047    0.000    0.047    0.000 multiarray.py:312(where)\n",
      "   392630    0.039    0.000    0.039    0.000 {method 'random' of '_random.Random' objects}\n",
      "        1    0.021    0.021    7.369    7.369 <string>:1(<module>)\n",
      "     4970    0.005    0.000    0.008    0.000 random.py:223(_randbelow)\n",
      "     4970    0.004    0.000    0.013    0.000 random.py:255(choice)\n",
      "     8641    0.002    0.000    0.002    0.000 {method 'getrandbits' of '_random.Random' objects}\n",
      "4975/4972    0.001    0.000    0.001    0.000 {built-in method builtins.len}\n",
      "     4970    0.001    0.000    0.001    0.000 {method 'bit_length' of 'int' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
      "        1    0.000    0.000    7.369    7.369 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 element_data.py:63(is_valid)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:2957(get_indexer)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.000    0.000    0.000    0.000 element_data.py:80(to_iloc)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
      "        1    0.000    0.000    0.000    0.000 graph.py:774(node_ids_to_ilocs)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3901(values)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4821(_maybe_promote)\n",
      "        1    0.000    0.000    0.000    0.000 explorer.py:81(_validate_walk_params)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:22(is_real_iterable)\n",
      "        1    0.000    0.000    0.000    0.000 element_data.py:69(require_valid)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:997(_handle_fromlist)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 common.py:862(is_dtype_equal)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:1743(is_extension_array_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1814(is_boolean)\n",
      "        1    0.000    0.000    0.000    0.000 graph.py:484(nodes)\n",
      "        1    0.000    0.000    0.000    0.000 abc.py:180(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 _asarray.py:16(asarray)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:47(_all)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:1850(_get_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 dtypes.py:83(find)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1012(_ndarray_values)\n",
      "        2    0.000    0.000    0.000    0.000 validation.py:59(require_integer_in_range)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        1    0.000    0.000    0.000    0.000 _weakrefset.py:70(__contains__)\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_platform_int}\n",
      "        1    0.000    0.000    0.000    0.000 missing.py:75(clean_fill_method)\n",
      "        1    0.000    0.000    0.000    0.000 missing.py:574(clean_reindex_fill_method)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:723(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5707(ensure_index)\n",
      "        5    0.000    0.000    0.000    0.000 explorer.py:42(_default_if_none)\n",
      "        1    0.000    0.000    0.000    0.000 explorer.py:477(_check_weights)\n",
      "        2    0.000    0.000    0.000    0.000 element_data.py:187(ids)\n",
      "        1    0.000    0.000    0.000    0.000 element_data.py:47(pandas_index)\n",
      "        1    0.000    0.000    0.000    0.000 element_data.py:54(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 explorer.py:65(_get_random_state)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "\n",
    "rw = BiasedRandomWalk(graph)\n",
    "\n",
    "cProfile.runctx(\n",
    "    \"walks = rw.run(graph.nodes(), n=2, length=walk_length, p=p, q=q)\",\n",
    "    locals(),\n",
    "    globals(),\n",
    "    sort='tottime'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of random walks for 'Train Graph': 24850\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U5'), dtype('<U5')) -> dtype('<U5')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-673a72292ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode2vec_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Train Graph\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3ca7325b2b80>\u001b[0m in \u001b[0;36mnode2vec_embedding\u001b[0;34m(graph, name)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2-stellar/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/anaconda3/envs/tf2-stellar/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2-stellar/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m             trim_rule=trim_rule, **kwargs)\n\u001b[1;32m    942\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2-stellar/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[0;34m(self, hs, negative, wv, update, vocabulary)\u001b[0m\n\u001b[1;32m   1874\u001b[0m         \u001b[0;31m# set initial input/projection and hidden weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1877\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2-stellar/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m(self, hs, negative, wv)\u001b[0m\n\u001b[1;32m   1891\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;31m# construct deterministic seed from word AND seed argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1893\u001b[0;31m             \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U5'), dtype('<U5')) -> dtype('<U5')"
     ]
    }
   ],
   "source": [
    "embedding_train = node2vec_embedding(graph_train, \"Train Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the link prediction model\n",
    "\n",
    "There are a few steps involved in using the Word2Vec model to perform link prediction:\n",
    "1. We calculate link/edge embeddings for the positive and negative edge samples by applying a binary operator on the embeddings of the source and target nodes of each sampled edge.\n",
    "2. Given the embeddings of the positive and negative examples, we train a logistic regression classifier to predict a binary value indicating whether an edge between two nodes should exist or not.\n",
    "3. We evaluate the performance of the link classifier for each of the 4 operators on the training data with node embeddings calculated on the **Train Graph** (`graph_train`), and select the best classifier.\n",
    "4. The best classifier is then used to calculate scores on the test data with node embeddings calculated on the **Test Graph** (`graph_test`).\n",
    "\n",
    "Below are a set of helper functions that let us repeat these steps for each of the binary operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 1. link embeddings\n",
    "def link_examples_to_features(link_examples, transform_node, binary_operator):\n",
    "    return [\n",
    "        binary_operator(transform_node(src), transform_node(dst))\n",
    "        for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "\n",
    "# 2. training classifier\n",
    "def train_link_prediction_model(\n",
    "    link_examples, link_labels, get_embedding, binary_operator\n",
    "):\n",
    "    clf = link_prediction_classifier()\n",
    "    link_features = link_examples_to_features(\n",
    "        link_examples, get_embedding, binary_operator\n",
    "    )\n",
    "    clf.fit(link_features, link_labels)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def link_prediction_classifier(max_iter=2000):\n",
    "    lr_clf = LogisticRegressionCV(Cs=10, cv=10, scoring=\"roc_auc\", max_iter=max_iter)\n",
    "    return Pipeline(steps=[(\"sc\", StandardScaler()), (\"clf\", lr_clf)])\n",
    "\n",
    "\n",
    "# 3. and 4. evaluate classifier\n",
    "def evaluate_link_prediction_model(\n",
    "    clf, link_examples_test, link_labels_test, get_embedding, binary_operator\n",
    "):\n",
    "    link_features_test = link_examples_to_features(\n",
    "        link_examples_test, get_embedding, binary_operator\n",
    "    )\n",
    "    score = evaluate_roc_auc(clf, link_features_test, link_labels_test)\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_roc_auc(clf, link_features, link_labels):\n",
    "    predicted = clf.predict_proba(link_features)\n",
    "\n",
    "    # check which class corresponds to positive links\n",
    "    positive_column = list(clf.classes_).index(1)\n",
    "    return roc_auc_score(link_labels, predicted[:, positive_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider 4 different operators: \n",
    "\n",
    "* *Hadamard*\n",
    "* $L_1$\n",
    "* $L_2$\n",
    "* *average*\n",
    "\n",
    "The paper [[1]](#refs) provides a detailed description of these operators. All operators produce link embeddings that have equal dimensionality to the input node embeddings (128 dimensions for our example). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operator_hadamard(u, v):\n",
    "    return u * v\n",
    "\n",
    "\n",
    "def operator_l1(u, v):\n",
    "    return np.abs(u - v)\n",
    "\n",
    "\n",
    "def operator_l2(u, v):\n",
    "    return (u - v) ** 2\n",
    "\n",
    "\n",
    "def operator_avg(u, v):\n",
    "    return (u + v) / 2.0\n",
    "\n",
    "\n",
    "def run_link_prediction(binary_operator):\n",
    "    clf = train_link_prediction_model(\n",
    "        examples_train, labels_train, embedding_train, binary_operator\n",
    "    )\n",
    "    score = evaluate_link_prediction_model(\n",
    "        clf,\n",
    "        examples_model_selection,\n",
    "        labels_model_selection,\n",
    "        embedding_train,\n",
    "        binary_operator,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"classifier\": clf,\n",
    "        \"binary_operator\": binary_operator,\n",
    "        \"score\": score,\n",
    "    }\n",
    "\n",
    "\n",
    "binary_operators = [operator_hadamard, operator_l1, operator_l2, operator_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [run_link_prediction(op) for op in binary_operators]\n",
    "best_result = max(results, key=lambda result: result[\"score\"])\n",
    "\n",
    "print(f\"Best result from '{best_result['binary_operator'].__name__}'\")\n",
    "\n",
    "pd.DataFrame(\n",
    "    [(result[\"binary_operator\"].__name__, result[\"score\"]) for result in results],\n",
    "    columns=(\"name\", \"ROC AUC score\"),\n",
    ").set_index(\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model using the test set\n",
    "\n",
    "Now that we've trained and selected our best model, we use a test set of embeddings and calculate a final evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_test = node2vec_embedding(graph_test, \"Test Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = evaluate_link_prediction_model(\n",
    "    best_result[\"classifier\"],\n",
    "    examples_test,\n",
    "    labels_test,\n",
    "    embedding_test,\n",
    "    best_result[\"binary_operator\"],\n",
    ")\n",
    "print(\n",
    "    f\"ROC AUC score on test set using '{best_result['binary_operator'].__name__}': {test_score}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise representations of link embeddings\n",
    "\n",
    "Learned link embeddings have 128 dimensions but for visualisation we project them down to 2 dimensions using the PCA algorithm ([link](https://en.wikipedia.org/wiki/Principal_component_analysis)). \n",
    "\n",
    "Blue points represent positive edges and red points represent negative (no edge should exist between the corresponding vertices) edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate edge features for test data\n",
    "link_features = link_examples_to_features(\n",
    "    examples_test, embedding_test, best_result[\"binary_operator\"]\n",
    ")\n",
    "\n",
    "# Learn a projection from 128 dimensions to 2\n",
    "pca = PCA(n_components=2)\n",
    "X_transformed = pca.fit_transform(link_features)\n",
    "\n",
    "# plot the 2-dimensional points\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.scatter(\n",
    "    X_transformed[:, 0],\n",
    "    X_transformed[:, 1],\n",
    "    c=np.where(labels_test == 1, \"b\", \"r\"),\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example has demonstrated how to use the `stellargraph` library to build a link prediction algorithm for homogeneous graphs using the Node2Vec, [1], representation learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "CloudRunner"
    ]
   },
   "source": [
    "<table><tr><td>Run the master version of this notebook:</td><td><a href=\"https://mybinder.org/v2/gh/stellargraph/stellargraph/master?urlpath=lab/tree/demos/link-prediction/random-walks/cora-lp-demo.ipynb\" alt=\"Open In Binder\" target=\"_parent\"><img src=\"https://mybinder.org/badge_logo.svg\"/></a></td><td><a href=\"https://colab.research.google.com/github/stellargraph/stellargraph/blob/master/demos/link-prediction/random-walks/cora-lp-demo.ipynb\" alt=\"Open In Colab\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a></td></tr></table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
